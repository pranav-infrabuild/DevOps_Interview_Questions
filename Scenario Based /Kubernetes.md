## Kubernates Common Erros :-

### 1. `CrashLoopBackOff`

<details>

The `CrashLoopBackOff` status in Kubernetes indicates that a container in a Pod is repeatedly failing and being restarted by the kubelet. Here's a breakdown of the issue:

### What is `CrashLoopBackOff`?

- **Definition**: `CrashLoopBackOff` is a status indicating that a container in a Pod has failed to start correctly and is continuously crashing and being restarted by Kubernetes.
- **Behavior**: When a container crashes, Kubernetes will try to restart it. If the container fails repeatedly, Kubernetes applies an increasing backoff interval between restart attempts to avoid overwhelming the system.

### Common Causes

1. **Application Errors**: The application inside the container might be failing due to bugs, misconfigurations, or other runtime issues.
2. **Configuration Issues**: Incorrect configuration files, environment variables, or secrets might be causing the container to fail.
3. **Resource Limits**: The container might be exceeding resource limits like CPU or memory, leading to crashes.
4. **Dependency Failures**: The container might be dependent on other services or containers that are not available or functioning correctly.
5. **Incorrect Start Command**: The entry point or command defined for the container might be incorrect or failing.

### Troubleshooting Steps

1. **Check Logs**: Use `kubectl logs <pod-name> -c <container-name>` to view the logs and identify the cause of the crash.
   
   **Example**:
   ```bash
   kubectl logs my-pod -c my-container
   ```

2. **Describe Pod**: Use `kubectl describe pod <pod-name>` to get detailed information about the Pod and look for events or error messages.

   **Example**:
   ```bash
   kubectl describe pod my-pod
   ```

3. **Verify Configuration**: Ensure that all configuration files, environment variables, and secrets are correctly set up.

4. **Check Resource Limits**: Ensure that the resource limits and requests for the container are set appropriately and that the container is not being killed due to exceeding these limits.

5. **Inspect Docker Image**: Make sure the Docker image used for the container is built correctly and that it can run without issues in isolation.

6. **Adjust Restart Policy**: Review and adjust the restart policy or backoff settings if necessary, although this is usually a temporary solution while resolving the root cause.


</details>

### 2. `Failed to pull image" or "ImagePullBackOff`

<details>

When Kubernetes is unable to pull a container image, it typically results in an error and the Pod will not start properly. Here are common reasons for this issue and steps to troubleshoot:

### Common Reasons for Image Pull Issues

1. **Incorrect Image Name or Tag**: The image name or tag specified in your Pod or Deployment configuration may be incorrect or misspelled.
2. **Image Not Found**: The specified image may not exist in the container registry.
3. **Authentication Issues**: Kubernetes may not have the required credentials to access a private container registry.
4. **Registry Connectivity Issues**: There may be network issues preventing Kubernetes from reaching the container registry.
5. **Rate Limiting**: Some container registries impose rate limits on image pulls, which may cause failures if exceeded.

### Troubleshooting Steps

1. **Check Pod Events**: Use `kubectl describe pod <pod-name>` to check the events and error messages related to the image pull.

   **Example**:
   ```bash
   kubectl describe pod my-pod
   ```

   Look for messages like "Failed to pull image" or "ImagePullBackOff."

2. **Verify Image Name and Tag**: Ensure that the image name and tag are correctly specified in your Pod or Deployment YAML file. The format should be `repository/image:tag`.

   **Example**:
   ```yaml
   spec:
     containers:
     - name: my-container
       image: my-repo/my-image:latest
   ```

3. **Check Image Availability**: Verify that the image exists in the container registry and is accessible. You can try pulling the image manually using Docker:

   **Example**:
   ```bash
   docker pull my-repo/my-image:latest
   ```

4. **Verify Registry Credentials**:
   - If using a private registry, ensure that you have created a Kubernetes Secret with the registry credentials and configured it in your Pod or Deployment.

   **Example**:
   ```bash
   kubectl create secret docker-registry my-registry-secret \
     --docker-server=<registry-url> \
     --docker-username=<username> \
     --docker-password=<password> \
     --docker-email=<email>
   ```

   Then, reference the Secret in your Pod or Deployment YAML:

   **Example**:
   ```yaml
   spec:
     containers:
     - name: my-container
       image: my-repo/my-image:latest
     imagePullSecrets:
     - name: my-registry-secret
   ```

5. **Check Network Connectivity**: Ensure that your Kubernetes nodes have network access to the container registry. This might involve checking firewall rules, network policies, or DNS settings.

6. **Inspect Registry Limits**: If you suspect rate limiting, check the registry documentation or contact support to understand any rate limits that might be affecting your pulls.

By following these steps, you can identify and resolve issues related to pulling container images in Kubernetes.

</details>

### 3. `pod is in a `Pending` state`
<details>


When a pod is in a `Pending` state, it means that the pod has been accepted by the Kubernetes system but it is not yet running on any node in the cluster. 

### 1. **Insufficient Resources:**
   - **Issue:** The nodes in your cluster may not have enough CPU, memory, or other resources to schedule the pod.
   - **Solution:** 
     - Check the available resources in your nodes using `kubectl describe nodes`.
     - If resources are insufficient, consider scaling up your cluster by adding more nodes or resizing the existing ones.



You can gather more information about why the pod is in a `Pending` state by running the following command:
```bash
kubectl describe pod <pod-name>
```
This will provide detailed information, including events that might explain why the pod is not being scheduled.
</details>

### 4. `Pod spec contains invalid configurations`
<details>

An invalid pod specification error typically occurs when there are issues in the YAML configuration of the pod. This might include syntax errors, missing required fields, or incorrect values.

### Troubleshooting Steps

1. **Review YAML Syntax:**
   - **Issue:** YAML is sensitive to indentation and formatting. Even a small mistake can cause errors.
   - **Solution:** 
     - Use a YAML validator or linting tool to check the syntax.
     - Ensure correct indentation, spacing, and structure.

2. **Check Required Fields:**
   - **Issue:** Missing required fields like `apiVersion`, `kind`, `metadata`, or `spec` can lead to errors.
   - **Solution:**
     - Ensure that the YAML file includes all the necessary fields. A basic pod spec should include:
       ```yaml
       apiVersion: v1
       kind: Pod
       metadata:
         name: my-pod
       spec:
         containers:
         - name: my-container
           image: nginx
       ```

3. **Verify Field Values:**
   - **Issue:** Incorrect values for fields (e.g., incorrect image name, invalid port number) can cause the pod to fail.
   - **Solution:**
     - Double-check all field values to ensure they are correct. For example, make sure the container image exists and the port numbers are valid.

4. **Use `kubectl apply --dry-run`:**
   - **Issue:** Applying the configuration without checking for errors can cause issues in the cluster.
   - **Solution:**
     - Use the `--dry-run=client` option to validate the pod spec without actually creating the pod:
       ```bash
       kubectl apply -f <pod-spec.yaml> --dry-run=client
       ```
     - This command will check for any errors in the YAML file and report them without applying the changes.

5. **Check for Typo in Fields:**
   - **Issue:** A typo in field names (e.g., `containters` instead of `containers`) can invalidate the specification.
   - **Solution:**
     - Carefully review the field names in the YAML file.


</details>

### 5. `Service Unavailable`
<details>

If you're encountering a "Service Unavailable" error in Kubernetes, it typically indicates that the service is not reachable or the backend Pods are not responding correctly. Here‚Äôs a step-by-step guide to troubleshoot this issue:

### 1. **Check Service Configuration**
   - Ensure that the service is correctly configured and running:
     ```bash
     kubectl get svc
     ```
   - Review the service configuration using:
     ```bash
     kubectl describe svc <service-name>
     ```
   - Ensure that the service type (ClusterIP, NodePort, LoadBalancer) is correctly set and the ports are properly configured.

### 2. **Verify Pods are Running**
   - Confirm that the backend Pods are running and ready to serve requests:
     ```bash
     kubectl get pods
     ```
   - Use `kubectl describe pod <pod-name>` to check the Pod's status, including readiness and liveness probes.

### 3. **Check Endpoint Readiness**
   - Ensure that the service has healthy endpoints:
     ```bash
     kubectl get endpoints <service-name>
     ```
   - Verify that the endpoints are correctly mapped to the Pods. If the endpoints list is empty, it indicates that no Pods match the service‚Äôs selector.

### 4. **Review Network Policies**
   - If network policies are implemented, make sure they are not blocking traffic between the service and the Pods:
     ```bash
     kubectl get networkpolicy
     ```
   - Review the policies using `kubectl describe networkpolicy <policy-name>` to ensure they allow traffic to the service.

### 5. **Check Service Connectivity**
   - Test connectivity to the service using `kubectl exec` to run commands inside a Pod:
     ```bash
     kubectl exec -it <pod-name> -- curl http://<service-name>:<port>
     ```
   - This command checks if the service is accessible from within the cluster.

### 6. **DNS Resolution**
   - Confirm that the service's DNS name is resolving correctly within the cluster:
     ```bash
     kubectl exec -it <pod-name> -- nslookup <service-name>
     ```
   - Ensure that the DNS setup is working properly if the service is not resolving.

### 7. **Review Service Logs**
   - Check the logs of the Pods backing the service to see if they are receiving requests and if any errors are occurring:
     ```bash
     kubectl logs <pod-name>
     ```

### 8. **Restart Pods or Service**
   - As a last resort, try restarting the Pods or the service to see if it resolves the issue:
     ```bash
     kubectl delete pod <pod-name>
     kubectl rollout restart deployment <deployment-name>
     ```

By following these steps, you should be able to diagnose and resolve the "Service Unavailable" issue in your Kubernetes cluster. If you need further assistance or have specific configurations you'd like to review, feel free to share them.

</details>

### Question 6. How would you define a Kubernetes Pod YAML to run a Spring Boot application as a single container exposing port 8080?

<details>

So Kubernetes needs to know:

1. **What kind of object** ‚Üí Pod
2. **Which image to run** ‚Üí Docker image
3. **Which port the app listens on** ‚Üí 8080


---

## Correct Pod YAML (Final Answer)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: springboot-app
spec:
  containers:
    - name: springboot-app-container
      image: myrepo/springboot-app:1.0
      ports:
        - containerPort: 8080
```

---

## Explanation in Very Simple Language

### 1Ô∏è‚É£ `apiVersion: v1`

- Tells Kubernetes which API version to use
- Pods use `v1`

### 2Ô∏è‚É£ `kind: Pod`

- Tells Kubernetes what you are creating
- Here ‚Üí a Pod

### 3Ô∏è‚É£ `metadata`

```yaml
metadata:
  name: springboot-app
```

- Name of the Pod
- Used to identify it in Kubernetes

### 4Ô∏è‚É£ `spec`

- Defines how the Pod should run

### 5Ô∏è‚É£ `containers`

```yaml
containers:
  - name: springboot-app-container
```

- A Pod can have multiple containers
- Here we have one container
- `-` means list item

### 6Ô∏è‚É£ `image`

```yaml
image: myrepo/springboot-app:1.0
```

- Docker image that contains your Spring Boot app

### 7Ô∏è‚É£ `ports`

```yaml
ports:
  - containerPort: 8080
```

- Tells Kubernetes: üëâ "This container listens on port 8080"
- Spring Boot runs on 8080 by default

‚ö†Ô∏è **Note:**
This does NOT expose the app outside. For external access, you need:
- Service (ClusterIP / NodePort / LoadBalancer)
- or Ingress

---


   
</details>


### Question 7: How do you inject environment variables from a ConfigMap into a Node.js Pod in Kubernetes?

<details>
---

## ‚úÖ Correct Pod YAML (Final Answer)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-app-pod
  labels:
    app: node-app
spec:
  containers:
    - name: app
      image: node:18
      envFrom:
        - configMapRef:
            name: node-env-config
```

---

## Very Simple Explanation (Real-time Scenario)

### Scenario

You have:
- A Node.js app
- It needs environment variables
- Variables are already stored in a ConfigMap
- ConfigMap name ‚Üí `node-env-config`
- You want Kubernetes to inject those variables into the container

---

## Key Part (Most Important)

```yaml
envFrom:
  - configMapRef:
      name: node-env-config
```

üëâ **This tells Kubernetes:**

> "Take all key-value pairs from the ConfigMap `node-env-config` and load them as environment variables inside my container."

### Example ConfigMap:

```yaml
NODE_ENV=production
DB_HOST=localhost
```

### Inside the Node.js App:

```javascript
process.env.NODE_ENV
process.env.DB_HOST
```

---

## Interview-friendly Explanation (1‚Äì2 Lines)

> "I use `envFrom` with `configMapRef` in the container spec to inject all environment variables from a ConfigMap into my Node.js Pod."

---
</details>


### Question 8 : How do you securely inject database credentials into a Pod using Kubernetes Secrets instead of hardcoding them?

<details>
---

## What This Task Means (Real-world)

You have:
- Database credentials (password)
- You must NOT hardcode them in YAML or code
- Kubernetes provides Secrets for sensitive data
- You want to inject the secret into the Pod as an environment variable

---


## ‚úÖ Correct & Secure YAML

**File:** `k8s_deployment_db_secret.yaml`

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  password: cGFzc3dvcmQ=
---
apiVersion: v1
kind: Pod
metadata:
  name: postgres-client
spec:
  containers:
    - name: db-client
      image: postgres
      env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
```

---

## Explanation in Very Simple Language

### 1Ô∏è‚É£ Kubernetes Secret

```yaml
kind: Secret
data:
  password: cGFzc3dvcmQ=
```

- Secrets store sensitive values
- Value is Base64 encoded
- `cGFzc3dvcmQ=` ‚Üí decodes to `password`

**Command to encode:**

```bash
echo -n password | base64
```

---

### 2Ô∏è‚É£ Inject Secret into Pod

```yaml
env:
  - name: DB_PASSWORD
    valueFrom:
      secretKeyRef:
        name: db-secret
        key: password
```

üëâ **This tells Kubernetes:**

> "Read the `password` from `db-secret` and set it as an environment variable called `DB_PASSWORD` inside the container."

---

### 3Ô∏è‚É£ Inside the Container

Your application can access it as:

**Linux / Shell**
```bash
echo $DB_PASSWORD
```

**Node.js**
```javascript
process.env.DB_PASSWORD
```

**Java / Spring Boot**
```java
System.getenv("DB_PASSWORD");
```

---

## Interview-ready Explanation (2 Lines)

> "To avoid hardcoding credentials, I store them in Kubernetes Secrets and inject them into the Pod using `env` with `secretKeyRef`, making the application secure and configurable."

---

## Why This is Secure ‚úÖ

- ‚úÖ No password in source code
- ‚úÖ No password in plain YAML
- ‚úÖ Can rotate secrets without code change
- ‚úÖ RBAC-controlled access

---


</details>

### Question 9: How do you define CPU and memory requests and limits for a Pod in Kubernetes?


<details>


---

## What the Requirement Says 

**Minimum guaranteed resources (requests):**
- CPU: 250m
- Memory: 256Mi

**Maximum allowed resources (limits):**
- CPU: 500m
- Memory: 512Mi

üëâ Kubernetes will **guarantee** the request  
üëâ Kubernetes will **not allow** usage beyond the limit

---


## ‚úÖ Correct Pod YAML (Final Answer)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-resources
  labels:
    app: resource-demo
spec:
  containers:
    - name: app
      image: node:18
      ports:
        - containerPort: 3000
      resources:
        requests:
          cpu: "250m"
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"
```

---

## Simple Explanation (Real-time Scenario)

### 1Ô∏è‚É£ requests

```yaml
requests:
  cpu: "250m"
  memory: "256Mi"
```

üëâ **Means:**
- Kubernetes **guarantees** this much resource
- Pod will be scheduled only on a node that has at least this much free

---

### 2Ô∏è‚É£ limits

```yaml
limits:
  cpu: "500m"
  memory: "512Mi"
```

üëâ **Means:**
- Pod **cannot exceed** this usage
- If memory exceeds ‚Üí Pod is **OOMKilled**
- If CPU exceeds ‚Üí Pod is **throttled**

---

## CPU & Memory Meaning (Very Simple)

- `250m` CPU = 0.25 CPU core
- `500m` CPU = 0.5 CPU core
- `Mi` = Mebibytes (used by Kubernetes)

---

## Interview-ready Explanation (2 Lines)

> "I define `resources.requests` to guarantee minimum CPU and memory for the Pod, and `resources.limits` to cap maximum usage, ensuring stable and fair resource allocation in Kubernetes."

---


</details>

### Question 10: How to keep environment-specific settings separate in Kubernetes

<details>
   
This is about:
- Having different configs for Test / Prod
- Same application & deployment YAML
- Different values like URL, DB host, log level
- Without changing code ‚ùå

---

‚úÖ **Separate ConfigMaps per environment**

### configmap-test.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_URL: "https://test.example.internal"
  DB_HOST: "test-db.internal"
  APP_MODE: "test"
  LOG_LEVEL: "debug"
```

### configmap-prod.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_URL: "https://prod.example.com"
  DB_HOST: "prod-db.internal"
  APP_MODE: "production"
  LOG_LEVEL: "info"
```

### üîë Key Idea

- **Same ConfigMap name:** `app-config`
- **Different values** per environment
- Application doesn't care which env it is running in
- Kubernetes injects correct config

---

## üîπ Why Same ConfigMap Name?

Because:
- Deployment YAML stays unchanged
- Only ConfigMap changes per environment
- Clean & scalable approach

---


‚úÖ **CI/CD driven deployment**

```bash
echo "Deploying to environment: $ENV"
```

### Step 1Ô∏è‚É£ Apply environment-specific ConfigMap

```bash
kubectl apply -f k8s/configmap-${ENV}.yaml
```

**Examples:**
- `ENV=test` ‚Üí `configmap-test.yaml`
- `ENV=prod` ‚Üí `configmap-prod.yaml`

### Step 2Ô∏è‚É£ Apply shared Deployment

```bash
kubectl apply -f k8s/deployment.yaml
```

‚úîÔ∏è Same deployment  
‚úîÔ∏è Same image  
‚úîÔ∏è Different configuration

---

## üîπ Real-world Flow (Simple)

| Environment | ConfigMap applied       |
|-------------|-------------------------|
| Test        | configmap-test.yaml     |
| Prod        | configmap-prod.yaml     |

**Deployment always uses:**

```yaml
envFrom:
  - configMapRef:
      name: app-config
```

---

## üîπ Why This is a BEST PRACTICE üöÄ

- ‚úÖ No hardcoding
- ‚úÖ Same YAML across environments
- ‚úÖ Easy rollback
- ‚úÖ CI/CD friendly
- ‚úÖ Clean separation of concerns

---

## üîπ Interview-ready Answer (Short)

> "We keep environment-specific settings separate by creating different ConfigMaps for each environment while using a shared Deployment YAML. During deployment, the pipeline applies the appropriate ConfigMap based on the environment."

---

## üîπ Interview-ready Answer (Strong)

> "I use environment-specific ConfigMaps like `configmap-test.yaml` and `configmap-prod.yaml` with the same name. The CI/CD pipeline applies the correct ConfigMap before deploying the application, ensuring clean configuration separation without changing deployment code."

---

</details>

### Question 10. You've deployed an application to Kubernetes, but it crashes immediately on startup with the error: "DB_HOST environment variable is not set" **Question:** What is the root cause for this?

<details>



### **Step 1: Observe the Application Failure**
When you run `kubectl get pods`, you see:
- Pod name: `my-app-7c9f8d6b5d-xkq9m`
- Status: `CrashLoopBackOff` 
- Ready: `0/1`
- Restarts: `4`

When you check the logs with `kubectl logs my-app-7c9f8d6b5d-xkq9m`, you see:
```
Error: DB_HOST environment variable is not set
```

**Conclusion:** The issue is configuration-related, not a problem with the application code itself.

---

### **Step 2: Verify the ConfigMap Exists and Has the Correct Keys**
You check if the ConfigMap exists:
```bash
kubectl get configmap app-config
```
Result: ConfigMap exists with 3 keys, age 10m

Then you inspect its contents:
```bash
kubectl describe configmap app-config
```
Result shows the ConfigMap contains:
- `DB_HOST: db.internal`
- `DB_PORT: 5432`
- `DB_USER: app_user`

**Conclusion:** The ConfigMap exists and has all the required configuration data. So the problem is NOT with ConfigMap creation.

---

### **Step 3: Inspect the Deployment Configuration**
You examine the deployment to see how it's configured:
```bash
kubectl get deployment my-app -o yaml
```

Looking at the relevant section:
```yaml
containers:
- name: app
  image: my-app:1.0
```

**Problem Found!** The deployment configuration shows the container with name and image, but there's NO reference to the ConfigMap. It's missing the `envFrom:` or `env:` section that would inject the ConfigMap values.

---

### **Step 4: Identify the Root Cause**
The diagram shows the workflow:
- ConfigMaps exist ‚úì
- Deployment exists ‚úì
- **But there's NO reference/connection between them** ‚úó

The deployment doesn't reference the ConfigMap via:
- `env` (individual environment variables)
- `envFrom` (all keys from ConfigMap)
- Volume mount

---

### **Step 5: Fix ‚Äî Inject the ConfigMap into the Pod**
Update the deployment to reference the ConfigMap:

```yaml
containers:
- name: app
  image: my-app:1.0
  envFrom:
  - configMapRef:
      name: app-config
```

This tells Kubernetes: "Take all the keys from the `app-config` ConfigMap and inject them as environment variables into the container."

---

### **Step 6: Verify the Fix**
After applying the fix with `kubectl apply -f deployment.yaml`:

1. Check pods: `kubectl get pods`
   - Status: `Running` ‚úì
   - Ready: `1/1` ‚úì

2. Check logs: `kubectl logs my-app-7c9f8d6b5d-r9k2p`
   - Output: `Application started successfully` ‚úì

The diagram shows the complete flow:
**ConfigMaps ‚Üí Application Config ‚Üí Deployment ‚Üí Application Started**

---

## Summary

**Root Cause:** The deployment was created without referencing the ConfigMap, so the environment variables (like DB_HOST) were never injected into the pod.

**Solution:** Add the `envFrom` section in the deployment YAML to reference the ConfigMap, allowing Kubernetes to inject those configuration values as environment variables into the container.

**Key Learning:** Creating a ConfigMap alone isn't enough‚Äîyou must explicitly reference it in your deployment configuration for the values to be available to your application.

---

## Additional Notes

### Three Ways to Use ConfigMaps:

1. **envFrom** (inject all keys as environment variables):
```yaml
envFrom:
- configMapRef:
    name: app-config
```

2. **env** (inject specific keys):
```yaml
env:
- name: DB_HOST
  valueFrom:
    configMapKeyRef:
      name: app-config
      key: DB_HOST
```

3. **Volume mount** (mount as files):
```yaml
volumes:
- name: config
  configMap:
    name: app-config
volumeMounts:
- name: config
  mountPath: /etc/config
```
</details>


### Question 11. A user tries to visit `admin.example.com` to access the Admin App, but instead they're getting the Main App!


<details>


- **Expected:** Admin App should load ‚úÖ
- **Actually happening:** Main App is loading ‚ùå
- **Both services are running fine** ‚úì
- **Using a single Ingress resource** ‚úì

**Question:** What mistake in the Ingress rules could cause this issue?

---

## Simple Explanation

Think of Ingress like a **traffic police officer** at a busy intersection. When a user types a website address, Ingress decides which application to send them to.

### **Real-World Analogy**
Imagine you have two restaurants:
- **Restaurant A** (Main App) - at "example.com"
- **Restaurant B** (Admin App) - at "admin.example.com"

A delivery person comes looking for "admin.example.com" but keeps getting sent to Restaurant A instead of Restaurant B. Why? The directions (rules) are in the wrong order!

---

## Step-by-Step Solution

### **Step 1: Observe the Problem**
When you test the website:
```bash
curl -i http://admin.example.com
```
Response: `HTTP/1.1 200 OK`

**What's happening:**
- User goes to `admin.example.com` ‚úì
- Ingress routes to Admin App ‚úì
- **BUT** when checking in reality, it's loading the Main App! ‚ùå

---

### **Step 2: Inspect the Ingress Rules**
Let's look at the Ingress configuration:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
spec:
  rules:
  # FIRST RULE
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: main-service
            port:
              number: 80
  
  # SECOND RULE
  - host: admin.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: admin-service
            port:
              number: 80
```

**Key observation:** Both rules use `path: /` with `pathType: Prefix`

---

### **Step 3: Identify the Root Cause**

**The Problem: Rules Order Matters!**

Here's what's happening:
- `example.com` is a **broader match** (matches ANY subdomain including admin.example.com)
- `admin.example.com` is a **more specific match**

**In Kubernetes Ingress, the FIRST matching rule wins!**

Since `example.com` is listed first and uses a catch-all path `/`, it matches requests to `admin.example.com` too!

**Think of it like this:**
```
User visits: admin.example.com
‚Üì
Rule 1: "example.com" with path "/" ‚Üí MATCHES! ‚úì (Too broad!)
‚Üì
Routes to: main-service ‚ùå (Wrong!)

Rule 2 never gets checked!
```

---

### **Step 4: Fix the Ingress Rule Order**

**Solution: Put the most specific rule FIRST!**

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
spec:
  rules:
  # FIRST RULE - Most specific (admin subdomain)
  - host: admin.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: admin-service
            port:
              number: 80
  
  # SECOND RULE - Less specific (main domain)
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: main-service
            port:
              number: 80
```

**What changed:** We **switched the order** of the rules!

---

### **Step 5: Verify the Fix**

After applying the corrected Ingress:

```bash
kubectl logs -n ingress-nginx \
  -l app.kubernetes.io/name=ingress-nginx \
  --tail=50
```

You should see:
```
"GET / HTTP/1.1" 200 ... "admin.example.com" ... upstream: admin-service:80
```

**Now when you visit `admin.example.com`, you see the Admin Dashboard!** ‚úÖ

---

## Summary

### **Root Cause**
The Ingress rules were in the **wrong order**. The broader rule (`example.com`) was checked first and matched everything, including `admin.example.com`.

### **Solution**
Reorder the rules so that **more specific rules come first**:
1. First: `admin.example.com` ‚Üí admin-service
2. Second: `example.com` ‚Üí main-service

### **Key Learning**
In Kubernetes Ingress:
- **Order matters!** Rules are checked from top to bottom.
- **Most specific rules should be listed first**
- Think of it like a checklist - once a match is found, it stops looking!

---

## Important Rules to Remember

### **Rule Ordering Best Practices**

1. **Most specific ‚Üí Least specific**
   ```
   ‚úì admin.example.com (very specific)
   ‚úì api.example.com (specific)
   ‚úì example.com (general)
   ```


</details>

### Question 12. If a backend service has **3 replicas**, Kubernetes might schedule:
- ‚ùå All 3 pods on **one node** **This is risky!**  If that node fails ‚Üí All pods go down ‚Üí Service outage ?


<details>



## ‚ùì Q1. What is Pod Anti-Affinity in Kubernetes?

**Answer:**

Pod Anti-Affinity is a **Kubernetes rule** that prevents pods from being scheduled together on the same node (or same zone).

üëâ It is mainly used to **spread replicas across nodes** to improve:
- ‚úÖ High availability
- ‚úÖ Fault tolerance

---

## ‚ùì Q2. What problem does Pod Anti-Affinity solve?

**Answer:**

By default, Kubernetes can schedule **multiple pod replicas on the same node**.

**If that node fails:**
- ‚ùå All pods on that node go down
- ‚ùå Service becomes unavailable

**Pod Anti-Affinity ensures:**
- ‚úÖ Pods are placed on **different nodes**
- ‚úÖ Failure of **one node** does not bring down the entire service

---

## ‚ùì Q3. How does Pod Anti-Affinity help?

**Answer:**

Using Pod Anti-Affinity, Kubernetes will try to:
- üéØ Place each replica on a **different node**
- üéØ Distribute workload across the cluster
- üéØ Prevent single point of failure

**Before Anti-Affinity:**
```
Node-1: [Pod-1, Pod-2, Pod-3] ‚ùå All eggs in one basket
Node-2: [ ]
Node-3: [ ]
```

**After Anti-Affinity:**
```
Node-1: [Pod-1] ‚úÖ
Node-2: [Pod-2] ‚úÖ
Node-3: [Pod-3] ‚úÖ
```

---

## ‚ùì Q4. What does `requiredDuringSchedulingIgnoredDuringExecution` mean?

**Answer:**

It means:

### üîπ **Required during scheduling**
‚Üí Pod will **NOT** be scheduled unless the rule is satisfied

### üîπ **Ignored during execution**
‚Üí Once the pod is running, Kubernetes will **not move it**, even if the rule is later broken

üëâ **Rule is checked only at scheduling time, not after.**

**Alternative:**
- `preferredDuringSchedulingIgnoredDuringExecution` ‚Üí Soft rule (best effort, not mandatory)

---

## ‚ùì Q5. Explain the Anti-Affinity YAML snippet in simple words

```yaml
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values:
          - my-backend
      topologyKey: "kubernetes.io/hostname"
```

**Answer:**

This rule says:

> üö´ **Do not place this pod on a node**  
> **If that node already has a pod with label `app=my-backend`**

**Use `hostname` to ensure one pod per node**

üëâ **Result:** Backend pods will be spread across different nodes

---

## ‚ùì Q6. What is `topologyKey: kubernetes.io/hostname`?

**Answer:**

`topologyKey` defines **where pods should not be placed together**.

| topologyKey | Meaning |
|-------------|---------|
| `kubernetes.io/hostname` | Different **nodes** |
| `topology.kubernetes.io/zone` | Different **zones** (e.g., us-east-1a, us-east-1b) |
| `topology.kubernetes.io/region` | Different **regions** (e.g., us-east, us-west) |

**Example:**
```yaml
# Spread across different availability zones
topologyKey: "topology.kubernetes.io/zone"
```

---

## ‚ùì Q7. Does Kubernetes allow all pods on the same node by default?

**Answer:**

‚úÖ **Yes**

By default, Kubernetes **does not prevent** multiple pods from running on the same node.

That's why **Anti-Affinity rules are needed** for high availability.

---

## ‚ùì Q8. Is this fault tolerance enabled by default?

**Answer:**

‚ùå **No.**

Kubernetes does **not guarantee** fault tolerance by default.

**You must explicitly configure:**
- ‚úÖ Pod Anti-Affinity
- ‚úÖ Pod Disruption Budgets (PDB)
- ‚úÖ Multiple nodes across zones
- ‚úÖ Resource requests and limits

---

## ‚ùì Q9. What happens if there are not enough nodes?

**Answer:**

If you use:
```yaml
requiredDuringSchedulingIgnoredDuringExecution
```

**And there are fewer nodes than replicas:**
- ‚ö†Ô∏è Some pods will remain in **Pending** state
- ‚ö†Ô∏è Because Kubernetes **cannot break the rule**

**Example:**
- 5 replicas with anti-affinity
- Only 3 nodes available
- Result: 3 pods running, 2 pods pending

**Solution:**
- Use `preferredDuringSchedulingIgnoredDuringExecution` (soft rule)
- Or add more nodes to the cluster

---

## ‚ùì Q10. When should you use Pod Anti-Affinity?

**Answer:**

Use it when:
- ‚úÖ Running **critical services**
- ‚úÖ Using **multiple replicas**
- ‚úÖ You want **high availability**
- ‚úÖ Node failure should **not affect all pods**

**Examples:**
- Database clusters (MongoDB, PostgreSQL)
- Microservices with replicas
- Stateful applications
- Mission-critical APIs

---

## üìã Complete YAML Example

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-backend
  template:
    metadata:
      labels:
        app: my-backend
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - my-backend
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: backend
        image: my-backend:v1.0
        ports:
        - containerPort: 8080
```

---

## üéØ Comparison: Required vs Preferred

| Feature | Required | Preferred |
|---------|----------|-----------|
| **Strictness** | Hard rule | Soft rule (best effort) |
| **Pod state if cannot satisfy** | Pending | Running (may ignore rule) |
| **Use case** | Critical HA requirements | Nice-to-have distribution |
| **Risk** | Pods may not schedule | All pods might end up on same node |

---

## üöÄ Advanced Example: Zone-Level Anti-Affinity

```yaml
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values:
          - my-backend
      topologyKey: "topology.kubernetes.io/zone"
```

**Result:** Each pod replica will be in a **different availability zone**

---

## üí° Best Practices

### ‚úÖ DO:
- Use anti-affinity for **critical stateful workloads**
- Combine with **Pod Disruption Budgets**
- Test failover scenarios
- Monitor pod distribution across nodes

### ‚ùå DON'T:
- Use `required` anti-affinity without enough nodes
- Forget to label your pods correctly
- Over-complicate affinity rules
- Ignore resource requests/limits

---

## ‚úÖ Best Interview Answer

> *"Pod Anti-Affinity prevents multiple replicas from being scheduled on the same node. By default, Kubernetes doesn't enforce this, so if a node fails, all pods could go down. I use `podAntiAffinity` with `requiredDuringSchedulingIgnoredDuringExecution` and `topologyKey: kubernetes.io/hostname` to ensure each replica runs on a different node. For critical services, I also use zone-level anti-affinity and Pod Disruption Budgets to maintain high availability during cluster maintenance or failures."*
</details>


### Question 13. You updated a Kubernetes Secret, but the pod is still using old values!** Why is this happening? How do you fix it?

<details>

## ‚ùì Q1. Why is the pod not reflecting new changes after updating a Secret?

**Answer:**

Kubernetes **does not automatically restart pods** when a Secret is updated.

### How Secrets are consumed:

| Method | Behavior After Update |
|--------|----------------------|
| **Environment variables** | ‚ùå Pod uses old value until restarted |
| **Mounted volume** | ‚úÖ File updates, but app may still need reload |

**The running pod continues using the old value until it is restarted.**

### Example:

```yaml
# Secret used as environment variable
env:
- name: DB_PASSWORD
  valueFrom:
    secretKeyRef:
      name: db-secret
      key: password
```
‚ùå Updating `db-secret` won't affect running pods automatically

```yaml
# Secret mounted as volume
volumeMounts:
- name: secret-volume
  mountPath: /etc/secrets
volumes:
- name: secret-volume
  secret:
    secretName: db-secret
```
‚úÖ File updates in ~60 seconds, but app needs to reload config

---

## ‚ùì Q2. You updated a Kubernetes Secret but the POD didn't pick the changes. What would you do?

**Answer:**

I would **restart the pod** so that it reloads the updated Secret.

‚ö†Ô∏è **The safest way is to restart the Deployment, not the pod manually.**

**Why?**
- ‚úÖ Ensures proper rolling update
- ‚úÖ Maintains high availability
- ‚úÖ Follows Kubernetes best practices
- ‚úÖ Triggers health checks
- ‚úÖ Updates all replicas consistently

---

## ‚ùì Q3. How do you restart pods in Kubernetes properly?

### ‚úÖ **Recommended Way (Deployment Restart)**

```bash
kubectl rollout restart deployment <deployment-name>
```

### **Explanation:**

When you run this command:
1. ‚úÖ Kubernetes creates **new pods** with updated configuration
2. ‚úÖ Old pods are **terminated gradually** (rolling update)
3. ‚úÖ Ensures **zero downtime**
4. ‚úÖ New pods pick up:
   - Updated Secrets
   - Updated ConfigMaps
   - New environment variables
   - Latest image (if tag is same)

### **Example:**

```bash
# Restart deployment named "my-app"
kubectl rollout restart deployment my-app

# Check rollout status
kubectl rollout status deployment my-app

# View rollout history
kubectl rollout history deployment my-app
```

**Output:**
```
deployment.apps/my-app restarted
Waiting for deployment "my-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "my-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "my-app" rollout to finish: 3 out of 3 new replicas have been updated...
deployment "my-app" successfully rolled out
```

---

## ‚ùì Q4. What happens when you delete a pod manually?

```bash
kubectl delete pod <pod-name>
```

**Answer:**

1. ‚úÖ The specified pod is **deleted immediately**
2. ‚úÖ If it belongs to a **Deployment/ReplicaSet**:
   - Kubernetes automatically creates a **new pod**
   - The new pod picks the **latest configuration**

### **Example:**

```bash
# Delete specific pod
kubectl delete pod my-app-5d8f7b9c4-xz7k2

# Kubernetes immediately creates a new pod
# New pod name: my-app-5d8f7b9c4-abc12
```

### ‚ö†Ô∏è **Important Notes:**

- ‚ùå This works, but it is **not the best practice** in production
- ‚ùå No controlled rollout
- ‚ùå No rollback capability
- ‚ùå Potential for brief downtime
- ‚ùå Not scalable for multiple pods
- ‚ùå Doesn't update deployment annotations

---

## ‚ùì Q5. Difference between `kubectl rollout restart` and `kubectl delete pod`?

| Feature | `kubectl rollout restart` | `kubectl delete pod` |
|---------|--------------------------|---------------------|
| **Method** | Controlled rolling update | Manual pod deletion |
| **Best Practice** | ‚úÖ Yes | ‚ùå No |
| **Zero Downtime** | ‚úÖ Guaranteed | ‚ö†Ô∏è Possible downtime |
| **Affects All Replicas** | ‚úÖ Yes | ‚ùå No (only specified pod) |
| **Rollback Support** | ‚úÖ Yes | ‚ùå No |
| **Production Safe** | ‚úÖ Yes | ‚ö†Ô∏è Not recommended |
| **Scalability** | ‚úÖ Works for all pods | ‚ùå Manual, not scalable |
| **Deployment History** | ‚úÖ Creates revision | ‚ùå No history |
| **Use Case** | Production deployments | Quick testing/debugging |

### üéØ **Interview Line:**

> *"I prefer restarting the deployment instead of deleting pods manually because it provides a controlled rollout with zero downtime and maintains deployment history for rollbacks."*


   
</details>
